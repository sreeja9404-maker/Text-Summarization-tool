pip install nltk
import nltk
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')
import re
from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize, word_tokenize
from collections import defaultdict # Simplifies frequency counting
import heapq # Selects top scoring sentences

def summarize_text(text, num_sentences=3): # Function definition ( Defalut summary length = 3 )
   
    # Removes special characters and extra spaces
    formatted_text = re.sub('[^a-zA-Z]', ' ', text)
    formatted_text = re.sub('\\s+', ' ', formatted_text)

    # Tokenize the text into sentences
    sentences = sent_tokenize(text)
    
    # Check if there are enough sentences to summarize
    if len(sentences) <= num_sentences:
        return text

    # Tokenize words and remove stopwords
    stop_words = set(stopwords.words('english'))
    word_frequencies = defaultdict(int)
    
    for word in word_tokenize(formatted_text.lower()):
        if word not in stop_words:
             word_frequencies[word] += 1

    # Calculate weighted frequencies ( Uses highest word frequency )
    max_frequency = max(word_frequencies.values()) if word_frequencies else 1

    for word in word_frequencies.keys():
        word_frequencies[word] = (word_frequencies[word] / max_frequency) # Normalizes the values between 0 & 1

    # Calculate sentence scores based on word frequencies
    sentence_scores = defaultdict(int)
    for sentence in sentences:
        for word in word_tokenize(sentence.lower()):
            if word in word_frequencies:
                sentence_scores[sentence] += word_frequencies[word]

    # Gets the top N sentences with the highest scores
    # The heapq.nlargest function is efficient for this purpose
    summary_sentences = heapq.nlargest(num_sentences, sentence_scores, key=sentence_scores.get)

    # Join the selected sentences to form the final summary
    summary = ' '.join(summary_sentences)
    return summary

# Example
long_text = """
Artificial Intelligence is transforming industries across the globe. It enables machines to perform tasks that normally require human intelligence, such as learning and reasoning.
Machine learning is a subset of artificial intelligence. It allows systems to learn from data and improve performance without explicit programming.
AI also raises ethical concerns including bias, privacy issues, and job displacement.
"""

summary_result = summarize_text(long_text, num_sentences=2)
print(f"Original Text Length: {len(long_text.split())} words")
print(f"Summary Length: {len(summary_result.split())} words")
print("\nGenerated Summary:")
print(summary_result)
